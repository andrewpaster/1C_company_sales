{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Future Sales Data for Shop-Item Pairs\n",
    "\n",
    "This is a data set from a Kaggle competition representing sales for various physical stores over approximately 2 year period. The items for sale at each store vary by month. \n",
    "\n",
    "For this model the following techniques were used:\n",
    "* encoding total shop sales from previous months as features\n",
    "* encoding total item sales from previous months as features\n",
    "* stacking machine learning algorithms \n",
    "\n",
    "This code currently gets approximately MSE 1.01 on the test set. At this point, the best way to reduce MSE would be with more feature selection especially text data about the stores and items.\n",
    "\n",
    "Acknowledgement - some of the data preparation code is from the Coursera-Kaggle machine learning competition course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test.csv.gz', 'sample_submission.csv.gz', 'items.csv', 'item_categories.csv', 'shops.csv', 'sales_train.csv.gz']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBModel\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import itertools\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "c64ffaa629757d97f314ac25c52fb1a1107caa5b"
   },
   "outputs": [],
   "source": [
    "# save memory by downcasting\n",
    "def downcast_dtypes(df):\n",
    "    '''\n",
    "        Changes column types in the dataframe: \n",
    "                \n",
    "                `float64` type to `float32`\n",
    "                `int64`   type to `int32`\n",
    "    '''\n",
    "    \n",
    "    # Select columns to downcast\n",
    "    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
    "    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n",
    "    \n",
    "    # Downcast\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols]   = df[int_cols].astype(np.int32)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "45911227a182dd586e13d4efa58269c20b092d6e"
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "sales = pd.read_csv('../input/sales_train.csv.gz')\n",
    "test = pd.read_csv('../input/test.csv.gz')\n",
    "items = pd.read_csv('../input/items.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "366008a33a77dee7cd5877a4dd6293e41190a37b"
   },
   "outputs": [],
   "source": [
    "# get all monthly shop/item pairs\n",
    "index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "\n",
    "grid = []\n",
    "\n",
    "for block in sales.date_block_num.unique():\n",
    "    cur_stores = sales.loc[sales.date_block_num == block, 'shop_id'].unique()\n",
    "    cur_items = sales.loc[sales.date_block_num == block, 'item_id'].unique()\n",
    "    grid.append(np.array(list(itertools.product(*[cur_stores, cur_items, [block]])), dtype='int32'))\n",
    "    \n",
    "grid = pd.DataFrame(np.vstack(grid), columns = index_cols, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f0a1080eedbe86e048d7d0a5a09a12cd3cb055ec"
   },
   "outputs": [],
   "source": [
    "# Groupby data to get shop-item-month aggregates\n",
    "gb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target':'sum'}})\n",
    "# Fix column names\n",
    "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values] \n",
    "# Join it to the grid\n",
    "all_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\n",
    "\n",
    "# Same as above but with shop-month aggregates\n",
    "gb = sales.groupby(['shop_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_shop':'sum'}})\n",
    "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n",
    "\n",
    "# Same as above but with item-month aggregates\n",
    "gb = sales.groupby(['item_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_item':'sum'}})\n",
    "gb.columns = [col[0] if col[-1] == '' else col[-1] for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n",
    "\n",
    "# Average price for the month for that store, item combination\n",
    "gb = sales.groupby(['shop_id', 'item_id', 'date_block_num'], as_index=False).agg({'item_price':{'avg_price':'mean'}})\n",
    "gb.columns = [col[0] if col[-1] == '' else col[-1] for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=index_cols).fillna(0)\n",
    "\n",
    "# Downcast dtypes from 64 to 32 bit to save memory\n",
    "all_data = downcast_dtypes(all_data)\n",
    "\n",
    "del grid, gb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d67c6a3fd22f2178d66711e9965a27961f9ac250"
   },
   "outputs": [],
   "source": [
    "# calculate lag data\n",
    "\n",
    "# List of columns that we will use to create lags\n",
    "cols_to_rename = list(all_data.columns.difference(index_cols)) \n",
    "\n",
    "shift_range = [1, 2, 3, 12, 24]\n",
    "\n",
    "test['date_block_num'] = 34\n",
    "\n",
    "for month_shift in tqdm_notebook(shift_range):\n",
    "    train_shift = all_data[index_cols + cols_to_rename].copy()\n",
    "    \n",
    "    train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n",
    "    \n",
    "    foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n",
    "    train_shift = train_shift.rename(columns=foo)\n",
    "\n",
    "    # get test data ready\n",
    "    test_result = pd.merge(test, train_shift, on=index_cols, how='left').fillna(0)\n",
    "    all_result = pd.merge(all_data, train_shift, on=index_cols, how='left').fillna(0)\n",
    "    \n",
    "    all_result = downcast_dtypes(all_result)\n",
    "    test_result = downcast_dtypes(test_result)\n",
    "    \n",
    "    all_result.to_csv('all_' + str(month_shift) + '.csv', index=False)\n",
    "    test_result.to_csv('test_' + str(month_shift) + '.csv', index=False)\n",
    "    \n",
    "    del train_shift, all_result, test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ab8d268be4d3439a48d62217fc6a780d36046f16"
   },
   "outputs": [],
   "source": [
    "lr1 = LinearRegression()\n",
    "lr2 = Ridge(normalize=True)\n",
    "lr3 = ElasticNet(normalize=True)\n",
    "lr4 = BayesianRidge(normalize=True)\n",
    "lr5 = LinearRegression()\n",
    "\n",
    "models = [lr1, lr2, lr3, lr4, lr5]\n",
    "\n",
    "X_train_level2 = np.array([])\n",
    "X_test_level2 = np.array([])\n",
    "\n",
    "for idx, month_shift in tqdm_notebook(enumerate(shift_range)):\n",
    "    training = pd.read_csv('all_' + str(month_shift) + '.csv')\n",
    "    testing = pd.read_csv('test_' + str(month_shift) + '.csv')\n",
    "    model = models[idx]\n",
    "    \n",
    "    training = training[training['date_block_num'] >= 12]\n",
    "    \n",
    "    # List of all lagged features\n",
    "    if month_shift == 12:\n",
    "        fit_cols = [col for col in training.columns if col[-2:] in [str(month_shift)]]\n",
    "    else:\n",
    "        fit_cols = [col for col in training.columns if col[-1:] in [str(month_shift)]] \n",
    "\n",
    "    # We will drop these at fitting stage\n",
    "    to_drop_cols = list(set(list(training.columns)) - (set(fit_cols)|set(index_cols))) + ['date_block_num'] \n",
    "    \n",
    "    # Category for each item\n",
    "    item_category_mapping = items[['item_id','item_category_id']].drop_duplicates()\n",
    "\n",
    "    training = pd.merge(training, item_category_mapping, how='left', on='item_id')\n",
    "    training = downcast_dtypes(training)\n",
    "\n",
    "    testing = pd.merge(testing, item_category_mapping, how='left', on='item_id')\n",
    "    testing = downcast_dtypes(testing)\n",
    "    \n",
    "    # Save `date_block_num`, as we can't use them as features, but will need them to split the dataset into parts \n",
    "    dates = training['date_block_num']\n",
    "    last_block = 34\n",
    "\n",
    "    dates_train = dates[dates <  last_block]\n",
    "\n",
    "    X_train = training.drop(to_drop_cols, axis=1)\n",
    "    X_test =  testing.drop(['date_block_num', 'ID'], axis=1)\n",
    "\n",
    "    y_train = training['target'].values\n",
    "    y_test =  None\n",
    "    \n",
    "    model.fit(X_train.values, y_train)\n",
    "    \n",
    "    pred = model.predict(X_test.values)\n",
    "    \n",
    "    if X_test_level2.size == 0:\n",
    "        X_test_level2 = np.zeros([testing.shape[0], len(shift_range)])\n",
    "    X_test_level2[:,idx] = pred\n",
    "        \n",
    "    print('Train rmse for model is %f' % np.sqrt(mean_squared_error(y_train, model.predict(X_train))))\n",
    "\n",
    "    # training data for final stacked model\n",
    "    dates_train_level2 = dates_train[dates_train.isin([27, 28, 29, 30, 31, 32, 33])]\n",
    "\n",
    "    # That is how we get target for the 2nd level dataset\n",
    "    y_train_level2 = y_train[dates_train.isin([27, 28, 29, 30, 31, 32, 33])]\n",
    "\n",
    "    # And here we create 2nd level feature matrix, init it with zeros first\n",
    "    if X_train_level2.size == 0:\n",
    "        X_train_level2 = np.zeros([y_train_level2.shape[0], len(shift_range)])\n",
    "\n",
    "    # Now fill X_train_level2`with metafeatures\n",
    "    current_row = 0\n",
    "\n",
    "    for cur_block_num in [24, 25, 26, 27, 28, 29, 30, 31, 32, 33]:\n",
    "\n",
    "        print(cur_block_num)\n",
    "\n",
    "        '''\n",
    "            1. Split `X_train` into parts\n",
    "               Remember, that corresponding dates are stored in `dates_train` \n",
    "            2. Fit linear regression \n",
    "            3. Fit LightGBM and put predictions          \n",
    "            4. Store predictions from 2. and 3. in the right place of `X_train_level2`. \n",
    "               You can use `dates_train_level2` for it\n",
    "               Make sure the order of the meta-features is the same as in `X_test_level2`\n",
    "        '''      \n",
    "\n",
    "        #  YOUR CODE GOES HERE\n",
    "        X_train_part = X_train[dates_train < cur_block_num]\n",
    "        y_train_part = y_train[dates_train < cur_block_num]\n",
    "\n",
    "        X_val_part = X_train[dates_train == cur_block_num]\n",
    "\n",
    "        model.fit(X_train_part, y_train_part)\n",
    "        pred_train = model.predict(X_val_part)\n",
    "        \n",
    "        X_train_level2[current_row:current_row + X_val_part.shape[0], idx] = pred_train\n",
    "\n",
    "        current_row += X_val_part.shape[0]    \n",
    "    \n",
    "    del training, testing, model, X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b7be2a34936e696c114bc8fc5aa072143cbab1c0"
   },
   "outputs": [],
   "source": [
    "# fit the level 2 model\n",
    "\n",
    "level2model = XGBModel(learning_rate=0.6)\n",
    "level2model.fit(X_train_level2, y_train_level2)\n",
    "\n",
    "train_preds = level2model.predict(X_train_level2)\n",
    "rmse_train_stacking = np.sqrt(mean_squared_error(y_train_level2, level2model.predict(X_train_level2)))\n",
    "print(rmse_train_stacking)\n",
    "test_preds = level2model.predict(X_test_level2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5cc8a26b29547658347ee2a2ed3a9c3f2272e9fb"
   },
   "outputs": [],
   "source": [
    "# output the results to disk\n",
    "\n",
    "np.savetxt('X_train_level2.csv', X_train_level2, delimiter=',')\n",
    "np.savetxt('y_train_level2.csv', y_train_level2, delimiter=',')\n",
    "np.savetxt('X_test_level2.csv', X_test_level2, delimiter=',')\n",
    "pd.DataFrame({'ID': test.ID, 'item_cnt_month':np.clip(test_preds, 0, 20)}).to_csv('results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Root env",
   "language": "python",
   "name": "root"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
